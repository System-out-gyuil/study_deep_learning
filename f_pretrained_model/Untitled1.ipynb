{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c955b26-d225-479c-acd8-260313463d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7387f2c-684a-44c0-835a-1a58e71c0572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75b663-3169-4a65-8b48-2206f4ca8ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.utils import shuffle\n",
    "import cv2\n",
    "\n",
    "IMAGE_SIZE = 112\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "class Dataset(Sequence):\n",
    "    def __init__(self, file_paths, targets, batch_size=BATCH_SIZE, aug=None, preprocess=None, shuffle=False):\n",
    "        self.file_paths = file_paths\n",
    "        self.targets = targets\n",
    "        self.batch_size = batch_size\n",
    "        self.aug = aug\n",
    "        self.preprocess = preprocess\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        if self.shuffle:\n",
    "            # 에포크 종료 시, 객체 생성 및 데이터 섞기\n",
    "            self.on_epoch_end()\n",
    "\n",
    "    # __len__()는 전체 데이터 건수에서 batch_size 단위로 나눈 데이터 수\n",
    "    # 예를 들어, 1000개의 데이터를 30 batch_size로 설정하면, 1 batch당 33.33..개이다.\n",
    "    # 이 때, 소수점은 무조건 올려서 33 + 1 = 34개로 설정한다.\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.targets) / self.batch_size))\n",
    "\n",
    "    # batch_size 단위로 이미지 배열과 타켓 데이터들을 가져온 뒤 변환한 값을 리턴한다.\n",
    "    def __getitem__(self, index):\n",
    "        file_paths_batch = self.file_paths[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        targets_batch = self.targets[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "\n",
    "        results_batch = np.zeros((file_paths_batch.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "        for i in range(file_paths_batch.shape[0]):\n",
    "            image = cv2.cvtColor(cv2.imread(file_paths_batch[i]), cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "            if self.aug is not None:\n",
    "                image = self.aug(image=image)['image']\n",
    "\n",
    "            if self.preprocess is not None:\n",
    "                image = self.preprocess(image)\n",
    "                    \n",
    "            results_batch[i] = image\n",
    "\n",
    "        return results_batch, targets_batch\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.file_paths, self.targets = shuffle(self.file_paths, self.targets)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb38056d-40c0-48d4-8c81-05ddca92e18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from tensorflow.keras.applications.xception import preprocess_input as xception_preprocess_input\n",
    "\n",
    "# train 데이터의 파일 경로를 리스트로 담아줌\n",
    "train_file_paths = train_df['file_paths'].values\n",
    "# target을 get_dummies를 통해 원핫인코딩\n",
    "train_targets = pd.get_dummies(train_df['targets']).values\n",
    "\n",
    "validation_file_paths = validation_df['file_paths'].values\n",
    "validation_targets = pd.get_dummies(validation_df['targets']).values\n",
    "\n",
    "test_file_paths = test_df['file_paths'].values\n",
    "test_targets = pd.get_dummies(test_df['targets']).values\n",
    "\n",
    "# aug = A.Compose([\n",
    "#     A.VerticalFlip(p=0.5),\n",
    "#     A.HorizontalFlip(p=0.5)\n",
    "# ])\n",
    "\n",
    "train_dataset = Dataset(train_file_paths, \n",
    "                        train_targets, \n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        preprocess=xception_preprocess_input, \n",
    "                        shuffle=True)\n",
    "\n",
    "validation_dataset = Dataset(validation_file_paths, \n",
    "                        validation_targets, \n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        preprocess=xception_preprocess_input)\n",
    "\n",
    "test_dataset = Dataset(test_file_paths, \n",
    "                        test_targets, \n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        preprocess=xception_preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9522c379-5495-45e5-b4cf-cc8c92946c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "def create_model(model_name='vgg16', verbose=False):\n",
    "    input_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    if model_name == 'vgg16':\n",
    "        model = VGG16(input_tensor=input_tensor, include_top=False, weights='imagenet')\n",
    "    elif model_name == 'resnet50': # ResNet50, 74.9% ; ResNet50V2, 76.0%\n",
    "        model = ResNet50V2(input_tensor=input_tensor, include_top=False, weights='imagenet')\n",
    "    elif model_name == 'xception': # Inception을 기초로 한 모델\n",
    "        model = Xception(input_tensor=input_tensor, include_top=False, weights='imagenet')\n",
    "    elif model_name == 'mobilenet':\n",
    "        model = MobileNetV2(input_tensor=input_tensor, include_top=False, weights='imagenet')\n",
    "\n",
    "    x = model.output\n",
    "\n",
    "    # 분류기\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    if model_name != 'vgg16':\n",
    "        x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(50, activation='relu')(x)\n",
    "    if model_name != 'vgg16':\n",
    "        x = Dropout(rate=0.5)(x)\n",
    "    output = Dense(100, activation='softmax', name='output')(x)\n",
    "    \n",
    "    model = Model(inputs=input_tensor, outputs=output)\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da938c96-8985-4a63-88fc-2aa2f21f0d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "\n",
    "model = create_model(model_name='xception', verbose=True)\n",
    "model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331aa7ff-d717-4a5f-b4a8-a9cb21176cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "mcp_cb = ModelCheckpoint(\n",
    "    filepath=\"./callback_files/weights.{epoch:03d}-{val_loss:.4f}-{acc:.4f}.weights.h5\",\n",
    "    monitor='val_loss',\n",
    "    save_best_only=False,\n",
    "    save_weights_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "rlr_cb = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "ely_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d74ca9-47ec-4a55-9288-be6ca6ed9789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# 메모리 해제 함수\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76924f77-57ac-4701-995e-083a9b42ddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 20\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=N_EPOCHS, \n",
    "                    validation_data=validation_dataset,\n",
    "                    callbacks=[mcp_cb, rlr_cb, ely_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e449f7-c810-443a-8c7a-0ef07c98f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d116c9-abd3-491e-b2a0-a3a7c74029f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg16_preprocess_input\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "IMAGE_SIZE = 112\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def fine_tune(datas, model_name, preprocess):\n",
    "    FIRST_EPOCHS = 10\n",
    "    SECOND_EPOCHS = 10\n",
    "    \n",
    "    train_file_paths, train_targets, \\\n",
    "    validation_file_paths, validation_targets, \\\n",
    "    test_file_paths, test_targets = datas\n",
    "\n",
    "    train_dataset = Dataset(train_file_paths, \n",
    "                        train_targets, \n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        preprocess=preprocess, \n",
    "                        shuffle=True)\n",
    "\n",
    "    validation_dataset = Dataset(validation_file_paths, \n",
    "                            validation_targets, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            preprocess=preprocess)\n",
    "    \n",
    "    model = create_model(model_name=model_name, verbose=True)\n",
    "    model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['acc'])\n",
    "\n",
    "    # feature extractor layer들을 전부 freeze\n",
    "    for layer in model.layers[:-5]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.fit(train_dataset, \n",
    "              batch_size=BATCH_SIZE, \n",
    "              epochs=FIRST_EPOCHS, \n",
    "              validation_data=validation_dataset)\n",
    "\n",
    "    # 배치 정규화만 freeze 진행\n",
    "    for layer in model.layers:\n",
    "        if not isinstance(layer, layers.BatchNormalization):\n",
    "            layer.trainable = True\n",
    "\n",
    "    # 부분 freeze 진행\n",
    "    for layer in model.layers[:14]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    model.compile(optimizer=Adam(0.00001), loss=CategoricalCrossentropy(), metrics=['acc'])\n",
    "    history = model.fit(train_dataset, \n",
    "              batch_size=BATCH_SIZE, \n",
    "              epochs=SECOND_EPOCHS, \n",
    "              validation_data=validation_dataset)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a8c348-de19-4c3d-a26c-746fd4159af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# 메모리 해제 함수\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8857de0-10ce-4028-82c0-39c0bf5f7d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from tensorflow.keras.applications.xception import preprocess_input as xception_preprocess_input\n",
    "\n",
    "train_file_paths = train_df['file_paths'].values\n",
    "# train_targets = train_df['targets'].values # SparseCategoricalCrossEntropy\n",
    "train_targets = pd.get_dummies(train_df['targets']).values # CategoricalCrossEntropy\n",
    "\n",
    "validation_file_paths = validation_df['file_paths'].values\n",
    "# validation_targets = validation_df['targets'].values # SparseCategoricalCrossEntropy\n",
    "validation_targets = pd.get_dummies(validation_df['targets']).values # CategoricalCrossEntropy\n",
    "\n",
    "model, history = fine_tune((train_file_paths, train_targets,\n",
    "           validation_file_paths, validation_targets,\n",
    "           test_file_paths, test_targets),\n",
    "          'xception', \n",
    "          xception_preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae37bab-1bf4-4a25-b1a7-495649aba805",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7875afc-014c-44eb-be54-9a9df5f8fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['acc'], label='train')\n",
    "plt.plot(history.history['val_acc'], label='validation')\n",
    "plt.ylim(0.4, 0.9)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877c719-c3b4-4502-8717-ef38ed0fabd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_scaling_for_train(image, mode='tf'):\n",
    "    aug = A.Compose([A.HorizontalFlip(p=0.5), \n",
    "                     A.VerticalFlip(p=0.5), \n",
    "                     A.OneOf([A.ColorJitter(p=1), \n",
    "                              A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1)], \n",
    "                             p=1)], p=0.5)\n",
    "    image = aug(image=image)['image']\n",
    "    \n",
    "    if mode == 'tf': # -1 ~ 1 scale\n",
    "        image = image / 127.5\n",
    "        image -= 1.\n",
    "    \n",
    "    elif mode == 'torch': # z-score scale\n",
    "        image = image / 255.\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        \n",
    "        image[:, :, 0] = (image[:, :, 0] - mean[0])/std[0]\n",
    "        image[:, :, 1] = (image[:, :, 1] - mean[1])/std[1]\n",
    "        image[:, :, 2] = (image[:, :, 2] - mean[2])/std[2]\n",
    "        \n",
    "    return image\n",
    "\n",
    "def preprocessing_scaling(image, mode='tf'):\n",
    "    if mode == 'tf': # -1 ~ 1 scale\n",
    "        image = image / 127.5\n",
    "        image -= 1.\n",
    "    \n",
    "    elif mode == 'torch': # z-score scale\n",
    "        image = image / 255.\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        \n",
    "        image[:, :, 0] = (image[:, :, 0] - mean[0])/std[0]\n",
    "        image[:, :, 1] = (image[:, :, 1] - mean[1])/std[1]\n",
    "        image[:, :, 2] = (image[:, :, 2] - mean[2])/std[2]\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106eb21d-7a43-4e41-adc9-26a702278607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "\n",
    "IMAGE_DIR = './datasets/butterfly_moth/test/'\n",
    "class_names = train_df.target_names.unique().tolist()\n",
    "\n",
    "def load_random_images(image_dir, class_names, num_images=20):\n",
    "    selected_classes = random.choices(class_names, k=num_images)\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for class_name in selected_classes:\n",
    "        class_dir = os.path.join(image_dir, class_name)\n",
    "        all_images = [os.path.join(class_dir, img) for img in os.listdir(class_dir) if img.endswith('.jpg')]\n",
    "        selected_image = random.choice(all_images)\n",
    "        images.append(selected_image)\n",
    "        labels.append(class_name)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def preprocess_image(image_path, image_size, mode='torch'):\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    resized_image = cv2.resize(image, (image_size, image_size))\n",
    "    preprocessed_image = preprocessing_scaling(resized_image, mode=mode)\n",
    "    preprocessed_image = np.expand_dims(preprocessed_image, axis=0)\n",
    "    return preprocessed_image\n",
    "\n",
    "random_images, random_labels = load_random_images(IMAGE_DIR, class_names, num_images=20)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "for idx, image_path in enumerate(random_images):\n",
    "    preprocessed_image = preprocess_image(image_path, IMAGE_SIZE, mode='torch')\n",
    "    predictions = model.predict(preprocessed_image)\n",
    "    # print(predictions[0])\n",
    "    predicted_index = np.argmax(predictions[0])\n",
    "    # predicted_class = int(predictions[0][0] < 0.5)\n",
    "\n",
    "    # print(predicted_index)\n",
    "    # print(class_names)\n",
    "    \n",
    "    predicted_class_name = class_names[predicted_index]\n",
    "    \n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    plt.subplot(4, 5, idx + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Predict: {predicted_class_name}\\nActual: {random_labels[idx]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "# print(class_names)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a295d8b-84e8-42a9-9057-f312c0db775c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e13d61-646c-475e-aec4-7c1896a4aea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92878b1-16a6-470b-8b95-5c5e0579946c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdd7091-440d-4972-bc3e-de438f814429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d3dc5a-6397-412b-a400-3a143f96f320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138072ad-3acd-4470-bf6f-4adf3bf0ce94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d9298e-95e0-4feb-bc49-6303cd3485c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a88a9-428f-426e-b923-a5f5552a84c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ced32-366e-4a55-a63b-8eeb6f726048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec6064f-9fa5-4af4-a113-6bdfdf8ae338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7188074c-6681-4470-9f48-ea3667972a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d9e490-cb77-4727-bf03-f011ab4c9e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3855301f-4e90-4f8f-ba7b-1b7c337126bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd10ba-64f1-443b-b2b3-10e982107eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaeb155-1b3b-4247-bd33-4d0eddd83d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8be8347-fd98-4fe5-aa70-61a525ea60d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608bd52e-118d-4ace-a7fc-d6bbe039ea94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
